parameters:
  job: null
  matrix:
    Production: {}
  pool:
    name: Hosted Ubuntu 1604
  imageName: null
  reportPrebuiltLeaks: false
  generatePrebuiltBurndown: false
  runUnitTests: false

jobs:
- job: ${{ parameters.job }}
  strategy:
    matrix: ${{ parameters.matrix }}
  pool: ${{ parameters.pool }}
  timeoutInMinutes: 270
  variables:
    # Prefix to distinguish artifacts from different legs.
    artifactName: ${{ format('{0} $(type)', parameters.job) }}
    # Use ":z" to set selinux flag for sharing in build-owned root dir. https://docs.docker.com/storage/bind-mounts/#configure-the-selinux-label
    docker.agentSrc.map: -v $(Build.SourcesDirectory):/agentSrc:z
    docker.agentSrc.work: -w /agentSrc
    docker.drop.map: -v $(dropDirectory):/drop:z
    docker.logs.map: -v $(dropDirectory)/logs:/logs:z
    docker.root.map: -v $(rootDirectory):/root:z
    docker.run: docker run --rm
    docker.src.map: -v $(rootDirectory)/sb/source-build:/src:z
    docker.src.work: -w /src
    docker.tb.map: -v $(rootDirectory)/sb/tarball:/tb:z
    docker.tb.work: -w /tb
    dropDirectory: $(stagingDirectory)/drop
    imageName: ${{ parameters.imageName }}
    reportPrebuiltLeaks: ${{ parameters.reportPrebuiltLeaks }}
    generatePrebuiltBurndown: ${{ parameters.generatePrebuiltBurndown }}
    rootDirectory: $(Build.SourcesDirectory)/..
    stagingDirectory: $(rootDirectory)/sb/staging
    tarballName: tarball_$(Build.BuildId)
    SOURCE_BUILD_SKIP_SUBMODULE_CHECK: true
    # Default type, can be overridden by matrix legs.
    type: Production

  steps:
  - template: ../steps/cleanup-unneeded-files.yml
  - template: ../steps/docker-cleanup-linux.yml
  - template: ../steps/calculate-config-flags.yml

  # Create working directory and copy source into it.
  - script: |
      set -ex
      df -h
      $(docker.run) $(docker.root.map) $(docker.agentSrc.map) $(docker.agentSrc.work) $(imageName) bash -c '
        rm -rf /root/sb/
        mkdir -p /root/sb/tarball
        cp -r . /root/sb/source-build' || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Clean sb directory and copy source from cloned directory

  - template: ../steps/init-submodules.yml
    parameters:
      commandPrefix: $(docker.run) $(docker.src.map) $(docker.src.work) $(imageName)

  # Build source-build.
  - script: |
      set -ex
      df -h
      if [ "$(sb.tarball)" != "true" ]; then
        failOnBaselineError=true
      fi
      $(docker.run) $(docker.src.map) $(docker.src.work) $(imageName) ./build.sh \
        /p:Configuration=$(sb.configuration) \
        /p:PortableBuild=$(sb.portable) \
        /p:ArchiveDownloadedPackages=$(sb.tarball) \
        /p:FailOnPrebuiltBaselineError=$failOnBaselineError \
        /p:ProdConBlobFeedUrlPrefix=$(prodConBlobFeedUrlPrefix) || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Build source-build
    timeoutInMinutes: 120

  # Generate prebuilt burndown data
  - script: |
      set -ex
      df -h
      $(docker.run) $(docker.src.map) $(docker.src.work) -e SOURCE_BUILD_SKIP_SUBMODULE_CHECK $(imageName) ./build.sh \
        --generate-prebuilt-data \
    displayName: Generate prebuilt burndown data
    condition: and(succeeded(), eq(variables['generatePrebuiltBurndown'], true))
    continueOnError: true

  # Run smoke tests. This is needed even in tarball legs to create the smoke-test-prereqs archive.
  - script: |
      set -ex
      df -h
      $(docker.run) $(docker.src.map) $(docker.src.work) $(imageName) ./build.sh \
        --run-smoke-test \
        /p:Configuration=$(sb.configuration) \
        /p:ProdConBlobFeedUrlPrefix=$(prodConBlobFeedUrlPrefix) || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Run smoke-test

  # Run unit tests that we support.
  - script: |
      set -ex
      df -h
      $(docker.run) $(docker.src.map) $(docker.src.work) $(imageName) ./build.sh \
        -test \
        /p:Configuration=$(sb.configuration) \
        /p:ProdConBlobFeedUrlPrefix=$(prodConBlobFeedUrlPrefix) || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Run unit tests
    condition: and(succeeded(), eq(variables['runUnitTests'], true))
    timeoutInMinutes: 40
    continueOnError: true

  # Create tarball.
  - script: |
      set -ex
      df -h
      args="--skip-build --minimize-disk-usage"
      if [ "$(reportPrebuiltLeaks)" = "true" ]; then
        args="$args --enable-leak-detection"
      fi
      $(docker.run) $(docker.tb.map) $(docker.src.map) $(docker.src.work) $(imageName) ./build-source-tarball.sh \
        "/tb/$(tarballName)" \
        $args || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Create tarball
    condition: and(succeeded(), eq(variables['sb.tarball'], true))

  # Copy logs from the production build
  - script: |
      set -ex
      df -h
      $(docker.run) $(docker.logs.map) $(docker.src.map) $(docker.src.work) $(imageName) /bin/bash -c "
        mkdir -p /logs/source-build/logs
        find . \( \
          -path './bin/*-report/*' -o \
          -path './bin/msbuild-debug/*' -o \
          -path './bin/roslyn-debug/*' -o \
          -iname '*.binlog' -o \
          -iname '*.log' \) \
          -exec cp {} --parents /logs/source-build/logs \;" || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Copy source-build production build logs
    condition: always()
    continueOnError: true

  # Delete key directories from local copy of repo to save space
  - script: |
      set -ex
      df -h
      sudo rm -rf $(rootDirectory)/sb/source-build/bin/src
      sudo rm -rf $(rootDirectory)/sb/source-build/bin/obj
      sudo rm -rf $(rootDirectory)/sb/source-build/.git
      sudo rm -rf $(rootDirectory)/sb/source-build/packages
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Delete key directories from source copy of repo to save space

  # tar the tarball directory into the drop directory.
  - script: |
      set -ex
      df -h
      $(docker.run) $(docker.tb.map) $(docker.drop.map) $(docker.tb.work) $(imageName) /bin/bash -c '
        mkdir -p /drop/tarball/
        smokeTestPackages="$(tarballName)/packages/smoke-test-packages"
        # smokeTestPackages is a package cache, with redundant data and unnecessary structure. E.g.
        # $smokeTestPackages/name/version/name.version.nupkg <- We want this.
        # $smokeTestPackages/name/version/lib/net46/name.dll <- This is already in the nupkg.
        # This find moves the nupkg files into $smokeTestPackages:
        find "$smokeTestPackages" -iname "*.nupkg" -exec mv {} "$smokeTestPackages" \;
        # This find removes all non-nupkg files, which are not wanted:
        find "$smokeTestPackages" -not -iname "*.nupkg" -delete
        # Make one .tar.gz for build, another for extras necessary to smoke test:
        tar --numeric-owner "--exclude=$smokeTestPackages" -zcf "/drop/tarball/$(tarballName).tar.gz" "$(tarballName)"
        tar --numeric-owner -zcf "/drop/tarball/$(tarballName)-smoke-test-prereqs.tar.gz" "$smokeTestPackages"' || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Copy tarball to output
    condition: and(succeeded(), eq(variables['sb.tarball'], true))

  # Build tarball.
  - script: |
      set -ex
      df -h
      networkArg=
      if [ "$(sb.tarballOffline)" = "true" ]; then
        networkArg="--network=none"
      fi
      poisonArg=
      if [ "$(reportPrebuiltLeaks)" = "true" ]; then
        poisonArg="/p:EnablePoison=true"
      fi
      $(docker.run) $(docker.tb.map) $(docker.tb.work) $networkArg $(imageName) "$(tarballName)/build.sh" \
        /p:Configuration=$(sb.configuration) \
        /p:PortableBuild=$(sb.portable) \
        /p:FailOnPrebuiltBaselineError=true \
        $poisonArg || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Build tarball
    timeoutInMinutes: 120
    condition: and(succeeded(), eq(variables['sb.tarball'], true))

  # Run smoke tests.
  - script: |
      set -ex
      df -h
      $(docker.run) $(docker.tb.map) $(docker.tb.work) $(imageName) "$(tarballName)/smoke-test.sh" \
        --minimal \
        --projectOutput \
        --configuration $(sb.configuration) \
        --prodConBlobFeedUrl '' || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Run smoke-test in tarball
    condition: and(succeeded(), eq(variables['sb.tarball'], true))

  - script: df -h
    displayName: Check space (df -h)
    condition: always()
    continueOnError: true

  # Copy logs and reports to staging directory.
  - script: |
      set -ex
      df -h
      $(docker.run) $(docker.logs.map) $(docker.tb.map) $(docker.tb.work) $(imageName) /bin/bash -c "
        mkdir -p /logs/tarball/logs
        cd \"$(tarballName)\"
        find . \( \
          -path './bin/*-report/*' -o \
          -path './bin/msbuild-debug/*' -o \
          -path './bin/roslyn-debug/*' -o \
          -iname '*.binlog' -o \
          -iname '*.log' \) \
          -exec cp {} --parents /logs/tarball/logs \;" || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Copy tarball logs
    condition: eq(variables['sb.tarball'], true)
    continueOnError: true

  # Copy source-built artifacts tarball to drop directory.
  - script: |
      set -ex
      df -h
      $(docker.run) $(docker.tb.map) $(docker.drop.map) $(docker.tb.work) $(imageName) /bin/bash -c "
        mkdir -p /drop/tarball/
        cd \"$(tarballName)\"
        find ./bin \( \
          -iname 'Private.SourceBuilt.Artifacts*.tar.gz' \) \
          -exec cp {} /drop/tarball/ \;" || exit 1
      du -h $(rootDirectory) | sort -h | tail -n 50
    displayName: Copy source-built artifacts tarball
    condition: eq(variables['sb.tarball'], true)
    continueOnError: true

  # Copy artifacts to staging - Copy to VSTS owned folder is done outside of docker so copied files
  # have correct ownership so VSTS can clean them up later.
  - task: CopyFiles@2
    condition: always()
    continueOnError: true
    inputs:
      sourceFolder: $(stagingDirectory)
      targetFolder: $(Build.ArtifactStagingDirectory)

  # Publish artifacts.
  - task: PublishBuildArtifacts@1
    displayName: Publish Logs artifact
    condition: always()
    continueOnError: true
    inputs:
      PathtoPublish: $(Build.ArtifactStagingDirectory)/drop/logs
      ArtifactName: Logs $(artifactName)
      ArtifactType: Container
  - task: PublishBuildArtifacts@1
    displayName: Publish Tarball artifact
    condition: eq(variables['sb.tarball'], true)
    continueOnError: true
    inputs:
      PathtoPublish: $(Build.ArtifactStagingDirectory)/drop/tarball
      ArtifactName: Tarball $(artifactName)
      ArtifactType: Container

  # Publish prebuilt report data to blob storage.
  - template: ../steps/publish-prebuilt-data-sh.yml
    parameters:
      relativeBlobPath: prebuilt-data/$(Build.SourceBranchName)/latest/$(artifactName)
  # Also publish per-build data to a unique location for future use.
  - template: ../steps/publish-prebuilt-data-sh.yml
    parameters:
      relativeBlobPath: prebuilt-data/$(Build.SourceBranchName)/rolling/$(Build.SourceVersion)/$(Build.BuildNumber)/$(Build.DefinitionName)/$(artifactName)

  # Clean up (very large) working directory. root owner makes it difficult for others to remove.
  - script: $(docker.run) $(docker.root.map) $(imageName) bash -c 'rm -rf /root/sb'
    displayName: Clean sb directory
    condition: always()
    continueOnError: true

  - template: ../steps/docker-cleanup-linux.yml

  - script: df -h
    displayName: Check space (df -h)
    condition: always()
    continueOnError: true
