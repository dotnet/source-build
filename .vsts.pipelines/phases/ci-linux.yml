parameters:
  dockerRegistryPassword: ''
  dockerRegistryServer: ''
  dockerRegistryUserName: ''
  matrix: null
  name: ci
  queueDemands: Agent.OS -equals Linux
  queueName: Hosted Ubuntu 1604

phases:
- phase: ${{ parameters.name }}
  variables:
    # Prefix to distinguish artifacts from different legs. No documented variable for this exists.
    artifactName: no_artifact_name
    buildLoggingOptions: ''
    buildConfiguration: Release
    buildOfflineTarball: false
    # Use ":z" to set selinux flag for sharing in build-owned root dir. https://docs.docker.com/storage/bind-mounts/#configure-the-selinux-label
    docker.agentSrc.map: -v $(Build.SourcesDirectory):/agentSrc:z
    docker.agentSrc.work: -w /agentSrc
    docker.drop.map: -v $(dropDirectory):/drop:z
    docker.logs.map: -v $(dropDirectory)/logs:/logs:z
    docker.root.map: -v $(rootDirectory):/root:z
    docker.run: docker run --rm
    docker.src.map: -v $(rootDirectory)/sb/source-build:/src:z
    docker.src.work: -w /src
    docker.tb.map: -v $(rootDirectory)/sb/tarball:/tb:z
    docker.tb.work: -w /tb
    dockerRegistry.password: ${{ parameters.dockerRegistryPassword }}
    dockerRegistry.server: ${{ parameters.dockerRegistryServer }}
    dockerRegistry.userName: ${{ parameters.dockerRegistryUserName }}
    dropDirectory: $(stagingDirectory)/drop
    rootDirectory: $(Build.SourcesDirectory)/..
    stagingDirectory: $(rootDirectory)/sb/staging
    tarballName: tarball_$(Build.BuildId)
  queue:
    name: ${{ parameters.queueName }}
    demands: ${{ parameters.queueDemands }}
    timeoutInMinutes: 240
    parallel: 2
    matrix: ${{ parameters.matrix }}
  steps:
  - template: ../steps/docker-cleanup-linux.yml

  # Docker registry login and pull, if one is defined.
  - script: |
      docker login -u $(dockerRegistry.userName) -p $(dockerRegistry.password) $(dockerRegistry.server)
      docker pull $(imageName)
    displayName: Docker login and image pull
    condition: and(succeeded(), ne(variables['dockerRegistry.server'], ''))

  # Docker registry logout, if one is defined. Do this immediately: avoid unnecessary login time.
  - script: docker logout $(dockerRegistry.server)
    displayName: Docker logout
    condition: ne(variables['dockerRegistry.server'], '')

  # Create working directory and copy source into it.
  - script: |
      set -x
      $(docker.run) $(docker.root.map) $(docker.agentSrc.map) $(docker.agentSrc.work) $(imageName) bash -c '
        rm -rf /root/sb/
        mkdir -p /root/sb/tarball
        cp -r . /root/sb/source-build'
    displayName: Clean sb directory and copy source from cloned directory

  # Fetch vsts commits if building internally.
  - script: |
      set -x
      # Ignore failure for the first command. It will intentionally fail if the commit is only
      # available in VSTS. "submodule update --init" is the simplest way to set up the submodule
      # directory. ("submodule init" only sets up .git/config, not the e.g. src/coreclr/.git and
      # .git/modules/src/coreclr/ directories.)
      $(docker.run) $(docker.src.map) $(docker.src.work) $(imageName) git submodule update --init --recursive
      $(docker.run) $(docker.src.map) $(docker.src.work) $(imageName) ./fetch-vsts-commits.sh $(user.PAT)
    displayName: Fetch internal vsts commits
    condition: and(succeeded(), ne(variables['user.PAT'], ''))

  # Initialize submodules.
  - script: $(docker.run) $(docker.src.map) $(docker.src.work) $(imageName) git submodule update --init --recursive
    displayName: Initialize submodules

  # Build source-build.
  - script: |
      $(docker.run) $(docker.src.map) $(docker.src.work) $(imageName) ./build.sh \
        /p:ArchiveDownloadedPackages=true \
        /p:Configuration=$(buildConfiguration) \
        /p:ContinueOnPrebuiltBaselineError=$(buildOfflineTarball) \
        /p:ProdConBlobFeedUrlPrefix=$(prodConBlobFeedUrlPrefix) \
        $(buildLoggingOptions)
    displayName: Build source-build
    timeoutInMinutes: 90

  # Copy logs to working directory.
  - script: |
      set -x
      $(docker.run) $(docker.logs.map) $(docker.src.map) $(docker.src.work) $(imageName) /bin/bash -c "
        mkdir -p /logs/source-build/logs
        find . \( \
          -iname '*.binlog' -o \
          -iname '*.log' \) \
          -exec cp {} --parents /logs/source-build/logs \;"
    displayName: Copy source-build logs
    condition: always()
    continueOnError: true

  # Run smoke tests.
  - script: |
      $(docker.run) $(docker.src.map) $(docker.src.work) $(imageName) ./build.sh \
        /t:RunSmokeTest \
        /p:Configuration=$(buildConfiguration) \
        /p:ProdConBlobFeedUrlPrefix=$(prodConBlobFeedUrlPrefix)
    displayName: Run smoke-test

  # Copy smoke test logs to working directory.
  - script: |
      $(docker.run) $(docker.logs.map) $(docker.src.map) $(docker.src.work) $(imageName) /bin/bash -c "
        mkdir -p /logs/source-build/smoke-test
        find ./testing-smoke -name '*.log' -exec cp {} /logs/source-build/smoke-test \;"
    displayName: Copy smoke-test logs
    condition: always()
    continueOnError: true

  # Create tarball.
  - script: |
      $(docker.run) $(docker.tb.map) $(docker.src.map) $(docker.src.work) $(imageName) ./build-source-tarball.sh \
        "/tb/$(tarballName)" \
        --skip-build
    displayName: Create tarball
    condition: and(succeeded(), eq(variables['buildOfflineTarball'], true))

  # tar the tarball directory into the drop directory.
  - script: |
      $(docker.run) $(docker.tb.map) $(docker.drop.map) $(docker.tb.work) $(imageName) /bin/bash -c '
        mkdir -p /drop/tarball/
        smokeTestPackages="$(tarballName)/prebuilt/smoke-test-packages"
        # smokeTestPackages is a package cache, with redundant data and unnecessary structure. E.g.
        # $smokeTestPackages/name/version/name.version.nupkg <- We want this.
        # $smokeTestPackages/name/version/lib/net46/name.dll <- This is already in the nupkg.
        # This find moves the nupkg files into $smokeTestPackages:
        find "$smokeTestPackages" -iname "*.nupkg" -exec mv {} "$smokeTestPackages" \;
        # This find removes all non-nupkg files, which are not wanted:
        find "$smokeTestPackages" -not -iname "*.nupkg" -delete
        # Make one .tar.gz for build, another for extras necessary to smoke test:
        tar --numeric-owner "--exclude=$smokeTestPackages" -zcf "/drop/tarball/$(tarballName).tar.gz" "$(tarballName)"
        tar --numeric-owner -zcf "/drop/tarball/$(tarballName)-smoke-test-prereqs.tar.gz" "$smokeTestPackages"'
    displayName: Copy tarball to output
    condition: and(succeeded(), eq(variables['buildOfflineTarball'], true))

  # Build tarball.
  - script: |
      $(docker.run) $(docker.tb.map) $(docker.tb.work) --network='none' $(imageName) "$(tarballName)/build.sh" \
        /p:Configuration=$(buildConfiguration) \
        $(buildLoggingOptions)
    displayName: Build tarball
    timeoutInMinutes: 90
    condition: and(succeeded(), eq(variables['buildOfflineTarball'], true))

  # Run smoke tests.
  - script: |
      $(docker.run) $(docker.tb.map) $(docker.tb.work) $(imageName) "$(tarballName)/smoke-test.sh" \
        --minimal \
        --projectOutput \
        --configuration $(buildConfiguration) \
        --prodConBlobFeedUrl ''
    displayName: Run smoke-test in tarball
    condition: and(succeeded(), eq(variables['buildOfflineTarball'], true))

  # Copy all tarball logs to working directory.
  - script: |
      set -x
      $(docker.run) $(docker.logs.map) $(docker.tb.map) $(docker.tb.work) $(imageName) /bin/bash -c "
        mkdir -p /logs/tarball/logs
        cd \"$(tarballName)\"
        find . \( \
          -iname '*.binlog' -o \
          -iname '*.log' \) \
          -exec cp {} --parents /logs/tarball/logs \;
        # Copy offline prebuilt report
        mkdir -p /logs/prebuilt-report/offline
        cp -r ./bin/prebuilt-report/* /logs/prebuilt-report/offline"
      $(docker.run) $(docker.logs.map) $(docker.src.map) $(docker.src.work) $(imageName) /bin/bash -c "
        # Copy online prebuilt report
        mkdir -p /logs/prebuilt-report/online
        cp -r ./bin/prebuilt-report/* /logs/prebuilt-report/online"
    displayName: Copy tarball logs
    condition: eq(variables['buildOfflineTarball'], true)
    continueOnError: true

  # Copy artifacts to staging - Copy to VSTS owned folder is done outside of docker so copied files
  # have correct ownership so VSTS can clean them up later.
  - task: CopyFiles@2
    condition: always()
    continueOnError: true
    inputs:
      sourceFolder: $(stagingDirectory)
      targetFolder: $(Build.ArtifactStagingDirectory)

  # Publish artifacts.
  - task: PublishBuildArtifacts@1
    displayName: Publish Logs artifact
    condition: always()
    continueOnError: true
    inputs:
      PathtoPublish: $(dropDirectory)/logs
      ArtifactName: Logs $(artifactName)
      ArtifactType: Container
  - task: PublishBuildArtifacts@1
    displayName: Publish Tarball artifact
    condition: eq(variables['buildOfflineTarball'], true)
    continueOnError: true
    inputs:
      PathtoPublish: $(dropDirectory)/tarball
      ArtifactName: Tarball $(artifactName)
      ArtifactType: Container

  # Clean up (very large) working directory. root owner makes it difficult for others to remove.
  - script: $(docker.run) $(docker.root.map) $(imageName) bash -c 'rm -rf /root/sb'
    displayName: Clean sb directory
    condition: always()
    continueOnError: true

  - template: ../steps/docker-cleanup-linux.yml
